{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Merlin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Merlin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb\n",
    "from nltk.corpus import stopwords\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import unidecode\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.animation as animation\n",
    "import operator\n",
    "import plotly.express as px\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCleaning(df):\n",
    "\n",
    "    #import pandas_profiling\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    df = df[['text','retweet_count','favorite_count']]\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    #Code to remove https\n",
    "    df['clean_tweet'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "    #Code to remove @\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([tweet for tweet in x.split()if not tweet.startswith(\"@\")]))\n",
    "    #Removing numbers\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))\n",
    "    #Removing all the greek characters using unidecode library\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()])) \n",
    "    #Removing the word 'hmm' and it's variants\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([word for word in x.split() if not word == 'h(m)+' ]))\n",
    "    #Code for removing slang words\n",
    "    d = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n",
    "                   'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n",
    "                   'cud':'could','fud':'food'} ## Need a huge dictionary\n",
    "    words = \"I luv myself\"\n",
    "    words = words.split()\n",
    "    reformed = [d[word] if word in d else word for word in words]\n",
    "    reformed = \" \".join(reformed)\n",
    "    \n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join(d[word] if word in d else word for word in x.split()))\n",
    "    #Finding words with # attached to it\n",
    "    df['#'] = df['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))\n",
    "    frame = df['#']\n",
    "    frame = pd.DataFrame(frame)\n",
    "    frame = frame.rename({'#':'Count(#)'},axis = 'columns')\n",
    "    frame[frame['Count(#)'] == ''] = 'No hashtags'\n",
    "    data_frame = pd.concat([df,frame],axis = 1)\n",
    "    data_frame.drop('#',axis = 1,inplace = True)\n",
    "    \n",
    "    #Column showing whether the corresponding tweet has a hash tagged word or not\n",
    "    data_frame = data_frame.rename({'Count(#)':'Hash words'},axis = 'columns')\n",
    "    \n",
    "    #Removing stopwords\n",
    "    data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(\n",
    "        lambda x : ' '.join([word for word in x.split() if not word in set(stopwords.words('english'))]))\n",
    "    #Lemmitization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    \n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    adwait = data_frame\n",
    "    #adwait.head()\n",
    "    data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    #reset index\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return data_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
